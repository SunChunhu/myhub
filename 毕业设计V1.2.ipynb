{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "associate-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import collections\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import tarfile\n",
    "import d2lzh_pytorch as d2l\n",
    "sys.path.append(\"..\")\n",
    "DATA_ROOT = \"/Users/air/Datasets\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-mountain",
   "metadata": {},
   "source": [
    "读取训练数据集和测试数据集\n",
    "' 1 ' ：正面\n",
    "' 0 ' : 负面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "virgin-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def read_imdb( folder = 'train', data_root = \"/Users/air/Datasets/aclImdb\"):\n",
    "    data = []\n",
    "    for lable in ['pos', 'neg']:\n",
    "        #分别读取训练数据集和测试数据集中的正面评论和负面评论，并打上标签\n",
    "        folder_name = os.path.join(data_root, folder, lable)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', ' ').lower()\n",
    "                data.append([review, 1 if lable=='pos' else 0])\n",
    "    #对所有数据进行随机排序\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-shame",
   "metadata": {},
   "source": [
    "对每条评论分词，这里使用空格分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exposed-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_imdb(data):\n",
    "    \"\"\"\n",
    "    data : list of [string, lable]\n",
    "    \"\"\"\n",
    "    def tokenizer(text):\n",
    "        #使用列表推捯式进行分词，并将所有单词小写\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    return [tokenizer(reviwe) for reviwe, _ in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-prior",
   "metadata": {},
   "source": [
    "根据分好词的训练数据集来创建词典，滤掉初现次数少于5的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessory-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_imdb(data):\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    \"\"\"\n",
    "    tokenized_data : [ [word1, word2, ...], [word3, word4, ...], ... ]\n",
    "    \"\"\"\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    \"\"\"\n",
    "    counter : dict of [word:num, ......]\n",
    "    \"\"\"\n",
    "    return Vocab.Vocab(counter, min_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-response",
   "metadata": {},
   "source": [
    "* 为了使每条评论的长度一致，通过截断或补零来使每条评论长度固定为500\n",
    "* vocab.stoi[] : [单词，索引]\n",
    "* vocab.itos[] : [索引，单词]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "classical-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    max_l = 500\n",
    "    \n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0]*(max_l - len(x))\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features =torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data]) \n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boxed-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.max_pool1d(x, kernel_size = x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "convinced-bidder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:02<00:00, 4472.49it/s]\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 4905.04it/s]\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 5663.80it/s]\n",
      "100%|██████████| 12500/12500 [00:03<00:00, 3988.39it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_data, test_data = read_imdb('train'), read_imdb('test')\n",
    "vocab = get_vocab_imdb(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "suburban-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "included-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "seasonal-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        self.pool = GlobalMaxPool1d()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels=2*embed_size,\n",
    "                                                       out_channels = c,\n",
    "                                                       kernel_size = k))\n",
    "    def forward(self, inputs):\n",
    "        embeddings = torch.cat((self.embedding(inputs),\n",
    "                                           self.constant_embedding(inputs)), dim=2)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        encoding = torch.cat([self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs],\n",
    "                                        dim=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-master",
   "metadata": {},
   "source": [
    " 从训练好的vocab中提取出words对应的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "brave-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    #生成与训练词向量维度相同的零矩阵\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0])\n",
    "    oov_count = 0\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 0\n",
    "    if oov_count > 0:\n",
    "        print(\"there are %d oov words\")\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "comparable-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=os.path.join(DATA_ROOT, \"glove\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lyric-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, kernal_sizes, nums_channels = 100, [2, 3, 4], [100, 100, 100]\n",
    "net = TextCNN(vocab, embed_size, kernal_sizes, nums_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alone-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embedding): Embedding(46152, 100)\n",
      "  (constant_embedding): Embedding(46152, 100)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (decoder): Linear(in_features=300, out_features=2, bias=True)\n",
      "  (pool): GlobalMaxPool1d()\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(200, 100, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(200, 100, kernel_size=(3,), stride=(1,))\n",
      "    (2): Conv1d(200, 100, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "optional-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.constant_embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.constant_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "banned-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "experienced-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: \n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "immediate-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, net.parameters()), lr = lr)\n",
    "loss  = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "affiliated-neighbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 0.4991, train acc 0.749, test acc 0.843, time 519.7 sec\n",
      "epoch 2, loss 0.1649, train acc 0.857, test acc 0.869, time 526.1 sec\n",
      "epoch 3, loss 0.0717, train acc 0.916, test acc 0.874, time 523.3 sec\n",
      "epoch 4, loss 0.0306, train acc 0.957, test acc 0.848, time 523.7 sec\n",
      "epoch 5, loss 0.0128, train acc 0.978, test acc 0.861, time 521.9 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-uruguay",
   "metadata": {},
   "source": [
    "batch_size=64;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "classified-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1 )\n",
    "    return 'positive' if label.item() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "impossible-louisiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "allied-reservoir",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-zambia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
